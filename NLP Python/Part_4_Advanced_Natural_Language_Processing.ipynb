{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_iCHKLuqhK_x"
      },
      "source": [
        "# Advanced Natural Language Processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SKrv_mcAhK_1"
      },
      "source": [
        "### Extracting Noun Phrases"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DBXqLVsDuomr",
        "outputId": "5d968e41-2ef2-45fc-cd70-5375a2b88823"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /gdrive; to attempt to forcibly remount, call drive.mount(\"/gdrive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd ../gdrive/My Drive/Colab Notebooks/_NLP/_nlp_data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TZtTmudYuopz",
        "outputId": "9faca7e3-c7bb-459d-8711-7e87e7e6b962"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Errno 2] No such file or directory: '../gdrive/My Drive/Colab Notebooks/_NLP/_nlp_data'\n",
            "/gdrive/My Drive/Colab Notebooks/_NLP/_nlp_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dZ3yuZtbhK_3",
        "outputId": "9fb21c30-5550-4af6-b40a-c3e2ca2d4c9f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "john\n",
            "natural language processing\n",
            "india\n",
            "google\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Package brown is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "#Import libraries\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('brown')\n",
        "from textblob import TextBlob\n",
        "\n",
        "#Extract noun\n",
        "blob = TextBlob(\"John is learning natural language processing in India and focused on Google\")\n",
        "for np in blob.noun_phrases:\n",
        "    print(np)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cr97o7TuhK_6",
        "outputId": "fd90bc7d-1988-46c0-f5e7-a3693d0bff68"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.06000000000000001\n",
            "-0.34166666666666673\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        }
      ],
      "source": [
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "text = '''\n",
        "The titular threat of The Blob has always struck me as the ultimate movie\n",
        "monster: an insatiably hungry, amoeba-like mass able to penetrate\n",
        "virtually any safeguard, capable of--as a doomed doctor chillingly\n",
        "describes it--\"assimilating flesh on contact.\n",
        "Snide comparisons to gelatin be damned, it's a concept with the most\n",
        "devastating of potential consequences, not unlike the grey goo scenario\n",
        "proposed by technological theorists fearful of\n",
        "artificial intelligence run rampant.\n",
        "'''\n",
        "\n",
        "blob = TextBlob(text)\n",
        "\n",
        "for sentence in blob.sentences:\n",
        "    print(sentence.sentiment.polarity)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7tfYvMPghK_6"
      },
      "outputs": [],
      "source": [
        "# blob.tags"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5sCg7wnBhK_7"
      },
      "source": [
        "### Finding Similarity Between Texts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o1wmwq1nhK_8",
        "outputId": "71723131-279c-485a-b7f3-a554e2a3286f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5, 10)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "documents = (\n",
        "\"I like NLP\",\n",
        "\"I am exploring NLP\",\n",
        "\"I am a beginn er in NLP\",\n",
        "\"I want to learn NLP\",\n",
        "\"I like advanced NLP\")\n",
        "\n",
        "#Import libraries\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "#Compute tfidf\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(documents)\n",
        "tfidf_matrix.shape\n",
        "#output\n",
        "(5, 10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c8oQAiozhK_8",
        "outputId": "6970dd52-596d-40b9-c483-1a371dc8f9b6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<1x11 sparse matrix of type '<class 'numpy.float64'>'\n",
              "\twith 2 stored elements in Compressed Sparse Row format>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "tfidf_matrix[0:1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q4HddwuPhK_9",
        "outputId": "f97b7ba9-b58f-4ec3-f4eb-5a3f49eb98a1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1.        , 0.17682765, 0.12305308, 0.13489366, 0.68374784]])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "#compute similarity for first sentence with rest of the sentences\n",
        "cosine_similarity(tfidf_matrix[0:1],tfidf_matrix)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wy_uNPzWhK_-"
      },
      "source": [
        "### Phonetic Matching "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wOo3s0tdhK_-",
        "outputId": "b4dc3db4-457d-46a7-c76b-2a13ec5cae5f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting fuzzy\n",
            "  Downloading Fuzzy-1.2.2.tar.gz (14 kB)\n",
            "Building wheels for collected packages: fuzzy\n",
            "  Building wheel for fuzzy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fuzzy: filename=Fuzzy-1.2.2-cp37-cp37m-linux_x86_64.whl size=164038 sha256=f19b1ea93108031b697b463055d8637ecb8c76d8ab384985c2e51fa165eecaac\n",
            "  Stored in directory: /root/.cache/pip/wheels/c8/52/8a/bb2d05fbf343752a8546682cb5b2d775cc0d1f27f6c43f95dd\n",
            "Successfully built fuzzy\n",
            "Installing collected packages: fuzzy\n",
            "Successfully installed fuzzy-1.2.2\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting soundex\n",
            "  Downloading soundex-1.1.3.tar.gz (9.1 kB)\n",
            "Collecting silpa_common>=0.3\n",
            "  Downloading silpa_common-0.3.tar.gz (9.4 kB)\n",
            "Building wheels for collected packages: soundex, silpa-common\n",
            "  Building wheel for soundex (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for soundex: filename=soundex-1.1.3-py3-none-any.whl size=8894 sha256=937eba301b2be9282e2cad6213a92f7f9bc5537cef4731ba2ba491d9fdd7d5fe\n",
            "  Stored in directory: /root/.cache/pip/wheels/6b/22/0f/0ac590ac1dc0fa60f57151bf39f1fae86a9978a1d54b48e197\n",
            "  Building wheel for silpa-common (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for silpa-common: filename=silpa_common-0.3-py3-none-any.whl size=8483 sha256=94aa08ae57cb887ae453499ec5616dfd23b7942f4d0f01ec444eddbe6307bb7c\n",
            "  Stored in directory: /root/.cache/pip/wheels/02/81/11/40ef1cf40f5c4021a214653ca3362914f0f9e14e8322f75f9b\n",
            "Successfully built soundex silpa-common\n",
            "Installing collected packages: silpa-common, soundex\n",
            "Successfully installed silpa-common-0.3 soundex-1.1.3\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting jellyfish\n",
            "  Downloading jellyfish-0.9.0.tar.gz (132 kB)\n",
            "\u001b[K     |████████████████████████████████| 132 kB 24.8 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: jellyfish\n",
            "  Building wheel for jellyfish (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for jellyfish: filename=jellyfish-0.9.0-cp37-cp37m-linux_x86_64.whl size=74020 sha256=4433bafb3fe6c5fcd46e32187ac2adc251bef0e18dc5c8827477eb289a9e9581\n",
            "  Stored in directory: /root/.cache/pip/wheels/fe/99/4e/646ce766df0d070b0ef04db27aa11543e2767fda3075aec31b\n",
            "Successfully built jellyfish\n",
            "Installing collected packages: jellyfish\n",
            "Successfully installed jellyfish-0.9.0\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[b'NTRL', None]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "!pip install fuzzy\n",
        "!pip install soundex\n",
        "!pip install jellyfish\n",
        "\n",
        "# https://www.informit.com/articles/article.aspx?p=1848528\n",
        "\n",
        "import fuzzy\n",
        "import soundex \n",
        "import jellyfish\n",
        "\n",
        "soundex = fuzzy.Soundex(4) \n",
        "dmetaphone = fuzzy.DMetaphone(4)\n",
        "dmetaphone('natural')\n",
        "\n",
        "# nysiis\n",
        "#Demaphone \n",
        "\n",
        "# soundex('natural') # throwing error - UnicodeDecodeError                        \n",
        "#soundex('natuaral')\n",
        "\n",
        "#ob = soundex.Soundex()\n",
        "#ob.soundex('Test')\n",
        "\n",
        "#jf = jellyfish.soundex(str('natural line')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kCFtYiAmhK__"
      },
      "source": [
        "### Tagging Part of Speech"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mx35jeN2hLAA",
        "outputId": "d2f5386c-1765-4ff6-dd1f-da38a40eef0e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Snide', 'NNP'),\n",
              " ('comparisons', 'NNS'),\n",
              " ('gelatin', 'NN'),\n",
              " ('damned', 'VBD'),\n",
              " (',', ','),\n",
              " (\"'s\", 'POS'),\n",
              " ('concept', 'NN'),\n",
              " ('devastating', 'VBG'),\n",
              " ('potential', 'JJ'),\n",
              " ('consequences', 'NNS'),\n",
              " (',', ','),\n",
              " ('unlike', 'IN'),\n",
              " ('grey', 'JJ'),\n",
              " ('goo', 'NN'),\n",
              " ('scenario', 'NN'),\n",
              " ('proposed', 'VBD'),\n",
              " ('technological', 'JJ'),\n",
              " ('theorists', 'NNS'),\n",
              " ('fearful', 'JJ'),\n",
              " ('artificial', 'JJ'),\n",
              " ('intelligence', 'NN'),\n",
              " ('run', 'NN'),\n",
              " ('rampant', 'NN'),\n",
              " ('.', '.')]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "Text  =  \"I love NLP and I will learn NLP in 2 month\"\n",
        "\n",
        "# Importing necessary packages and stopwords\n",
        "import nltk \n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords \n",
        "from nltk.tokenize import word_tokenize, sent_tokenize \n",
        "stop_words = set(stopwords.words('english')) \n",
        "\n",
        "# Tokenize the text\n",
        "tokens = sent_tokenize(text) \n",
        "\n",
        "#Generate tagging for all the tokens using loop\n",
        "for i in tokens: \n",
        "    words = nltk.word_tokenize(i) \n",
        "    words = [w for w in words if not w in stop_words]  \n",
        "    #  POS-tagger.  \n",
        "    tags = nltk.pos_tag(words) \n",
        "\n",
        "tags"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uaF2PDlihLAB",
        "outputId": "9a6a11dd-8d63-4f4f-8584-2c40a918b9ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/omw-1.4.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('Sukanya', 'NNP'), ('Rajib', 'NNP'), ('and', 'CC'), ('Naba', 'NNP'), ('are', 'VBP'), ('my', 'PRP$'), ('good', 'JJ'), ('friends', 'NNS'), ('Sukanya', 'NNP'), ('is', 'VBZ'), ('getting', 'VBG'), ('married', 'VBN'), ('next', 'JJ'), ('year', 'NN'), ('Marriage', 'NN'), ('is', 'VBZ'), ('a', 'DT'), ('big', 'JJ'), ('step', 'NN'), ('in', 'IN'), ('one', 'CD'), ('’', 'NN'), ('s', 'NN'), ('life.It', 'NN'), ('is', 'VBZ'), ('both', 'DT'), ('exciting', 'VBG'), ('and', 'CC'), ('frightening', 'NN'), ('But', 'CC'), ('friendship', 'NN'), ('is', 'VBZ'), ('a', 'DT'), ('sacred', 'JJ'), ('bond', 'NN'), ('between', 'IN'), ('people.It', 'NN'), ('is', 'VBZ'), ('a', 'DT'), ('special', 'JJ'), ('kind', 'NN'), ('of', 'IN'), ('love', 'NN'), ('between', 'IN'), ('us', 'PRP'), ('Many', 'JJ'), ('of', 'IN'), ('you', 'PRP'), ('must', 'MD'), ('have', 'VB'), ('tried', 'VBN'), ('searching', 'VBG'), ('for', 'IN'), ('a', 'DT'), ('friend', 'NN'), ('but', 'CC'), ('never', 'RB'), ('found', 'VBD'), ('the', 'DT'), ('right', 'JJ'), ('one', 'NN')]\n"
          ]
        }
      ],
      "source": [
        "# from textblob lib import TextBlob method\n",
        "from textblob import TextBlob\n",
        "nltk.download('omw-1.4')\n",
        "  \n",
        "text = (\"Sukanya, Rajib and Naba are my good friends. \" +\n",
        "    \"Sukanya is getting married next year. \" +\n",
        "    \"Marriage is a big step in one’s life.\" +\n",
        "    \"It is both exciting and frightening. \" + \n",
        "    \"But friendship is a sacred bond between people.\" +\n",
        "    \"It is a special kind of love between us. \" +\n",
        "    \"Many of you must have tried searching for a friend \"+ \n",
        "    \"but never found the right one.\")\n",
        "  \n",
        "# create a textblob object\n",
        "blob_object = TextBlob(text)\n",
        "  \n",
        "# Part-of-speech tags can be accessed \n",
        "# through the tags property of blob object.'\n",
        "  \n",
        "# print word with pos tag.\n",
        "print(blob_object.tags)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v8TlHpi7hLAC"
      },
      "source": [
        "### Extract Entities From Text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S-yDaYL9hLAC"
      },
      "outputs": [],
      "source": [
        "sent = \"John is studying at Stanford University in California\"\n",
        "\n",
        "#import libraries\n",
        "import nltk\n",
        "from nltk import ne_chunk\n",
        "from nltk import word_tokenize\n",
        "# nltk.download('maxent_ne_chunker')\n",
        "# nltk.download('words')\n",
        "\n",
        "\n",
        "#NER\n",
        "# ne_chunk(nltk.pos_tag(word_tokenize(sent)), binary=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vQmvIPL4hLAD"
      },
      "outputs": [],
      "source": [
        "# using spaCy for NER\n",
        "import spacy\n",
        "#nlp = spacy.load('en')\n",
        "nlp = spacy.blank(\"en\")\n",
        "\n",
        "\n",
        "# Read/create a sentence\n",
        "doc = nlp(u'Apple is ready to launch new phone worth $10000 in New york time square ')\n",
        "\n",
        "for ent in doc.ents:\n",
        "    print(ent.text, ent.start_char, ent.end_char, ent.label_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "09ljaGsbhLAE"
      },
      "source": [
        "### Extracting Topics From Text\n",
        "#### Document tagging and clustering "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s2W4whgihLAE",
        "outputId": "adf2b051-216f-471a-8d6a-9433ae39b8e4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.7/dist-packages (3.6.0)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (5.2.1)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.21.6)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.15.0)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.4.1)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[(0, 1), (1, 1), (2, 1), (3, 1), (4, 3), (5, 1), (6, 1)],\n",
              " [(6, 1), (7, 1), (8, 1), (9, 1), (10, 1)],\n",
              " [(11, 1), (12, 1), (13, 1), (14, 1), (15, 1)]]"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "doc1 = \"I am learning NLP, it is very interesting and exciting. it includes machine learning and deep learning\" \n",
        "doc2 = \"My father is a data scientist and he is nlp expert\"\n",
        "doc3 = \"My sister has good exposure into android development\"\n",
        "\n",
        "doc_complete = [doc1, doc2, doc3] \n",
        "doc_complete\n",
        "\n",
        "# Install and import libraries\n",
        "\n",
        "!pip install gensim\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "import string\n",
        "\n",
        "# Text preprocessing as discussed in chapter 2\n",
        "\n",
        "stop = set(stopwords.words('english'))\n",
        "exclude = set(string.punctuation) \n",
        "lemma = WordNetLemmatizer()\n",
        "def clean(doc):\n",
        "    stop_free = \" \".join([i for i in doc.lower().split() if i not in stop])\n",
        "    punc_free = ''.join(ch for ch in stop_free if ch not in exclude)\n",
        "    normalized = \" \".join(lemma.lemmatize(word) for word in punc_free.split())\n",
        "    return normalized\n",
        "\n",
        "doc_clean = [clean(doc).split() for doc in doc_complete]  \n",
        "doc_clean\n",
        "\n",
        "# Importing gensim\n",
        "import gensim\n",
        "from gensim import corpora\n",
        "\n",
        "# Creating the term dictionary of our corpus, where every unique term is   #assigned an index. \n",
        "dictionary = corpora.Dictionary(doc_clean)\n",
        "\n",
        "# Converting a list of documents (corpus) into Document-Term Matrix using #dictionary prepared above.\n",
        "doc_term_matrix = [dictionary.doc2bow(doc) for doc in doc_clean]\n",
        "doc_term_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TtzISQd3hLAG",
        "outputId": "514a3b78-0b2b-4af5-9d5e-a04850ba59e5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(0, '0.129*\"sister\" + 0.129*\"good\" + 0.129*\"exposure\" + 0.129*\"development\" + 0.129*\"android\" + 0.032*\"nlp\" + 0.032*\"father\" + 0.032*\"data\" + 0.032*\"expert\" + 0.032*\"scientist\"'), (1, '0.129*\"nlp\" + 0.129*\"father\" + 0.129*\"data\" + 0.129*\"scientist\" + 0.129*\"expert\" + 0.032*\"exposure\" + 0.032*\"development\" + 0.032*\"android\" + 0.032*\"good\" + 0.032*\"sister\"'), (2, '0.233*\"learning\" + 0.093*\"deep\" + 0.093*\"includes\" + 0.093*\"interesting\" + 0.093*\"machine\" + 0.093*\"exciting\" + 0.093*\"nlp\" + 0.023*\"scientist\" + 0.023*\"data\" + 0.023*\"father\"')]\n"
          ]
        }
      ],
      "source": [
        "# Creating the object for LDA model using gensim library\n",
        "Lda = gensim.models.ldamodel.LdaModel\n",
        "\n",
        "# Running and Training LDA model on the document term matrix for 3 topics.\n",
        "ldamodel = Lda(doc_term_matrix, num_topics=3, id2word = dictionary, passes=50)\n",
        "\n",
        "# Results\n",
        "print(ldamodel.print_topics())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ydb1TdihhLAH"
      },
      "source": [
        "### Classifying Text"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "QpWDhI3dv6mb",
        "outputId": "3e810e41-04ea-424b-b934-3530dcfb79fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/gdrive/MyDrive/Colab Notebooks/_NLP/_nlp_data'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "ov25POEwhLAH"
      },
      "outputs": [],
      "source": [
        "#Read the data\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Email_Data = pd.read_csv('spam.csv',sep='\\t')\n",
        "Email_Data.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "RtgKO-9ywNMJ",
        "outputId": "24ec614a-70b7-4a5b-e2ce-3b7e658336c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Type                                            Message\n",
              "0   ham  Go until jurong point, crazy.. Available only ...\n",
              "1   ham                      Ok lar... Joking wif u oni...\n",
              "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
              "3   ham  U dun say so early hor... U c already then say...\n",
              "4   ham  Nah I don't think he goes to usf, he lives aro..."
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-17559c68-29ba-42ea-9944-9265952db451\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Type</th>\n",
              "      <th>Message</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ham</td>\n",
              "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ham</td>\n",
              "      <td>Ok lar... Joking wif u oni...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>spam</td>\n",
              "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ham</td>\n",
              "      <td>U dun say so early hor... U c already then say...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ham</td>\n",
              "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-17559c68-29ba-42ea-9944-9265952db451')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-17559c68-29ba-42ea-9944-9265952db451 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-17559c68-29ba-42ea-9944-9265952db451');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Data undestanding\n",
        "Email_Data.columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qHGiqfZsv19B",
        "outputId": "c08b42ed-a485-4abf-b97e-e8eaa8ea3dbd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['Type', 'Message'], dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Email_Data = Email_Data.rename(columns={\"Type\":\"Target\", \"Message\":\"Email\"})"
      ],
      "metadata": {
        "id": "BWeqPKidv0Nb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "lNrH3GkihLAI",
        "outputId": "efb25b95-2df5-4bb4-ac45-58d3934ac3d9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "  Target                                              Email\n",
              "0    ham  go jurong point, crazy.. avail bugi n great wo...\n",
              "1    ham                        ok lar... joke wif u oni...\n",
              "2   spam  free entri 2 wkli comp win fa cup final tkt 21...\n",
              "3    ham          u dun say earli hor... u c alreadi say...\n",
              "4    ham              nah think goe usf, live around though"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-181b232e-c6af-4a4d-a661-8b056bdafa70\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Target</th>\n",
              "      <th>Email</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ham</td>\n",
              "      <td>go jurong point, crazy.. avail bugi n great wo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ham</td>\n",
              "      <td>ok lar... joke wif u oni...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>spam</td>\n",
              "      <td>free entri 2 wkli comp win fa cup final tkt 21...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ham</td>\n",
              "      <td>u dun say earli hor... u c alreadi say...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ham</td>\n",
              "      <td>nah think goe usf, live around though</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-181b232e-c6af-4a4d-a661-8b056bdafa70')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-181b232e-c6af-4a4d-a661-8b056bdafa70 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-181b232e-c6af-4a4d-a661-8b056bdafa70');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ],
      "source": [
        "#import\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import string\n",
        "from nltk.stem import SnowballStemmer\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "import os\n",
        "from textblob import TextBlob\n",
        "from nltk.stem import PorterStemmer\n",
        "from textblob import Word\n",
        "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
        "import sklearn.feature_extraction.text as text\n",
        "from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm\n",
        "\n",
        "#pre processing steps like lower case, stemming and lemmatization \n",
        "\n",
        "Email_Data['Email'] = Email_Data['Email'].apply(lambda x: \" \".join(x.lower() for x in x.split()))\n",
        "stop = stopwords.words('english')\n",
        "Email_Data['Email'] = Email_Data['Email'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
        "st = PorterStemmer()\n",
        "Email_Data['Email'] = Email_Data['Email'].apply(lambda x: \" \".join([st.stem(word) for word in x.split()]))\n",
        "Email_Data['Email'] =Email_Data['Email'].apply(lambda x: \" \".join([Word(word).lemmatize() for word in x.split()]))\n",
        "\n",
        "Email_Data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zfFnb_dohLAI",
        "outputId": "8abd7a4a-2ec8-40c1-8dcd-1c842b84c30f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.56514748, 0.60101303, 0.56514748, ..., 0.23140609, 0.27739829,\n",
              "       0.30891207])"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ],
      "source": [
        "#Splitting data into train and validation\n",
        "\n",
        "train_x, valid_x, train_y, valid_y = model_selection.train_test_split(Email_Data['Email'], Email_Data['Target'])\n",
        "\n",
        "# TFIDF feature generation for a maximum of 5000 features\n",
        "\n",
        "encoder = preprocessing.LabelEncoder()\n",
        "train_y = encoder.fit_transform(train_y)\n",
        "valid_y = encoder.fit_transform(valid_y)\n",
        "\n",
        "tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=5000)\n",
        "tfidf_vect.fit(Email_Data['Email'])\n",
        "xtrain_tfidf =  tfidf_vect.transform(train_x)\n",
        "xvalid_tfidf =  tfidf_vect.transform(valid_x)\n",
        "\n",
        "xtrain_tfidf.data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yU5gycaOhLAJ",
        "outputId": "1539fda5-948e-4091-f53d-0cdf06c588e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy:  0.975609756097561\n"
          ]
        }
      ],
      "source": [
        "def train_model(classifier, feature_vector_train, label, feature_vector_valid, is_neural_net=False):\n",
        "    # fit the training dataset on the classifier\n",
        "    classifier.fit(feature_vector_train, label)\n",
        "    # predict the labels on validation dataset\n",
        "    predictions = classifier.predict(feature_vector_valid)\n",
        "    return metrics.accuracy_score(predictions, valid_y)\n",
        "\n",
        "# Naive Bayes trainig\n",
        "accuracy = train_model(naive_bayes.MultinomialNB(alpha=0.2), xtrain_tfidf, train_y, xvalid_tfidf)\n",
        "print (\"Accuracy: \", accuracy)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "od9ts6WdhLAK",
        "outputId": "186408b4-00ee-4d06-ab05-e45d7ec73b20"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy:  0.8902439024390244\n"
          ]
        }
      ],
      "source": [
        "# Linear Classifier on Word Level TF IDF Vectors\n",
        "accuracy = train_model(linear_model.LogisticRegression(), xtrain_tfidf, train_y, xvalid_tfidf)\n",
        "print (\"Accuracy: \", accuracy)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QOghvYXmhLAK"
      },
      "source": [
        "### Carrying Out Sentiment Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Iys0sGNhLAK",
        "outputId": "ca739265-2689-4165-bf2b-2345ecee9659"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Sentiment(polarity=0.7, subjectivity=0.6000000000000001)"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ],
      "source": [
        "review = \"I like this phone. screen quality and camera clarity is really good.\"\n",
        "review2 = \"This tv is not good. Bad quality, no clarity, worst experience\"\n",
        "\n",
        "#import libraries\n",
        "from textblob import TextBlob\n",
        "\n",
        "#TextBlob has a pre trained sentiment prediction model\n",
        "blob = TextBlob(review)\n",
        "blob.sentiment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JdkIGBTWhLAK",
        "outputId": "d0ea6572-194b-4144-e6c9-c859c74d5be1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Sentiment(polarity=-0.6833333333333332, subjectivity=0.7555555555555555)"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ],
      "source": [
        "#now lets look at the sentiment of review2\n",
        "blob = TextBlob(review2)\n",
        "blob.sentiment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xThxtkfJhLAL"
      },
      "source": [
        "### Disambiguating Text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FwgDv7n6hLAM"
      },
      "outputs": [],
      "source": [
        "Text1 = 'I went to the bank to deposit my money'\n",
        "Text2 = 'The river bank was full of dead fishes'\n",
        "\n",
        "#Install pywsd\n",
        "\n",
        "!pip install pywsd\n",
        "\n",
        "#Import functions\n",
        "\n",
        "from nltk.corpus import wordnet as wn\n",
        "from nltk.stem import PorterStemmer\n",
        "from itertools import chain\n",
        "from pywsd.lesk import simple_lesk\n",
        "\n",
        "# Sentences\n",
        "\n",
        "bank_sents = ['I went to the bank to deposit my money',\n",
        "'The river bank was full of dead fishes']\n",
        "\n",
        "# calling the lesk function and printing results for both the sentences\n",
        "\n",
        "print (\"Context-1:\", bank_sents[0])\n",
        "answer = simple_lesk(bank_sents[0],'bank')\n",
        "print (\"Sense:\", answer)\n",
        "print (\"Definition : \", answer.definition())\n",
        "\n",
        "\n",
        "print (\"Context-2:\", bank_sents[1])\n",
        "answer = simple_lesk(bank_sents[1],'bank','n')\n",
        "print (\"Sense:\", answer)\n",
        "print (\"Definition : \", answer.definition())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "xMIil6WPhLAQ"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.10"
    },
    "colab": {
      "name": "Part 4 - Advanced Natural Language Processing.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}